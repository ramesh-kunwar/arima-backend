import os
import json
import uuid
import sys
from datetime import datetime
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd
import numpy as np
from arima_model import ARIMAForecaster
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Configuration
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MODELS_FOLDER'] = 'saved_models'
app.config['RESULTS_FOLDER'] = 'results'

# Create necessary directories
for folder in ['uploads', 'saved_models', 'results']:
    os.makedirs(folder, exist_ok=True)

# Global storage for training sessions
training_sessions = {}

def detect_date_format(date_series):
    """
    Helper function to detect and suggest date format
    """
    sample_dates = date_series.dropna().head(10).astype(str)
    formats_found = []
    
    for date_str in sample_dates:
        if '/' in date_str:
            parts = date_str.split('/')
            if len(parts[0]) == 4:
                formats_found.append('YYYY/MM/DD')
            elif len(parts) >= 2 and parts[0].isdigit() and int(parts[0]) > 12:
                formats_found.append('DD/MM/YYYY')
            else:
                formats_found.append('MM/DD/YYYY or DD/MM/YYYY')
        elif '-' in date_str:
            parts = date_str.split('-')
            if len(parts[0]) == 4:
                formats_found.append('YYYY-MM-DD')
            elif len(parts) >= 2 and parts[0].isdigit() and int(parts[0]) > 12:
                formats_found.append('DD-MM-YYYY')
            else:
                formats_found.append('MM-DD-YYYY or DD-MM-YYYY')
    
    return list(set(formats_found))

@app.route('/', methods=['GET'])
def health_check():
    """Health check endpoint"""
    default_csv_exists = os.path.exists('default_data.csv')
    return jsonify({
        'message': 'ARIMA Forecasting Service is running',
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0',
        'default_csv_available': default_csv_exists,
        'endpoints': {
            'upload': '/upload - Upload CSV and train model (supports file upload or uses default CSV)',
            'train_default': '/train-default - Train using default CSV file',
            'sessions': '/sessions - List all training sessions',
            'forecast': '/forecast/<session_id> - Get forecast results',
            'download': '/download/<session_id> - Download results as JSON'
        }
    })

@app.route('/upload', methods=['POST'])
def upload_and_train():
    """
    Upload CSV file and start ARIMA training
    Expected CSV format: date column + value columns
    If no file is uploaded, uses default CSV from root folder
    """
    try:
        # Check if file is uploaded
        file_uploaded = 'file' in request.files and request.files['file'].filename != ''
        
        if file_uploaded:
            file = request.files['file']
            if not file.filename.lower().endswith('.csv'):
                return jsonify({'error': 'Only CSV files are supported'}), 400
        
        # Get form parameters (can come from form data or JSON)
        if request.form:
            date_column = request.form.get('date_column', 'date')
            value_column = request.form.get('value_column', 'value')
            test_size = float(request.form.get('test_size', 0.2))
            forecast_horizon = int(request.form.get('forecast_horizon', 7))
            # Custom PDQ parameters
            custom_p = request.form.get('p')
            custom_d = request.form.get('d')
            custom_q = request.form.get('q')
        else:
            # Handle JSON request when no file is uploaded
            data = request.get_json() or {}
            date_column = data.get('date_column', 'date')
            value_column = data.get('value_column', 'value')
            test_size = float(data.get('test_size', 0.2))
            forecast_horizon = int(data.get('forecast_horizon', 7))
            # Custom PDQ parameters
            custom_p = data.get('p')
            custom_d = data.get('d')
            custom_q = data.get('q')
        
        # Build custom order if all PDQ values are provided
        custom_order = None
        if custom_p is not None and custom_d is not None and custom_q is not None:
            custom_order = (int(custom_p), int(custom_d), int(custom_q))
            logger.info(f"Using custom PDQ order: {custom_order}")
        
        session_id = str(uuid.uuid4())
        
        if file_uploaded:
            # Save uploaded file
            filename = secure_filename(file.filename)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{session_id}_{filename}")
            file.save(file_path)
            logger.info(f"File uploaded: {filename} with session_id: {session_id}")
        else:
            # Use default CSV from root folder
            default_csv_path = 'pizza-sales.csv'
            if not os.path.exists(default_csv_path):
                return jsonify({
                    'error': 'No file uploaded and no default CSV found. Please upload a CSV file or ensure default_data.csv exists in the root directory.'
                }), 400
            
            file_path = default_csv_path
            filename = 'default_data.csv'
            logger.info(f"Using default CSV: {filename} with session_id: {session_id}")
        
        # Load and validate data
        try:
            df = pd.read_csv(file_path)
            logger.info(f"CSV loaded with shape: {df.shape}")
            
            # Validate required columns
            if date_column not in df.columns:
                return jsonify({'error': f'Date column "{date_column}" not found in CSV'}), 400
            
            if value_column not in df.columns:
                return jsonify({'error': f'Value column "{value_column}" not found in CSV'}), 400
            
            # Data preprocessing with robust date parsing
            logger.info(f"Attempting to parse dates in column: {date_column}")
            
            # First, detect the date format for better error messages
            detected_formats = detect_date_format(df[date_column])
            logger.info(f"Detected date formats: {detected_formats}")
            
            try:
                # Try multiple date formats and parsing options
                df[date_column] = pd.to_datetime(df[date_column], format='mixed', dayfirst=True)
                logger.info("Successfully parsed dates using mixed format with dayfirst=True")
            except Exception as e:
                logger.warning(f"Mixed format parsing failed: {e}")
                try:
                    # Try with dayfirst=True for DD-MM-YYYY, DD/MM/YYYY formats
                    df[date_column] = pd.to_datetime(df[date_column], dayfirst=True, errors='coerce')
                    logger.info("Successfully parsed dates using dayfirst=True")
                except Exception as e2:
                    logger.warning(f"Dayfirst parsing failed: {e2}")
                    try:
                        # Try with infer_datetime_format
                        df[date_column] = pd.to_datetime(df[date_column], infer_datetime_format=True, errors='coerce')
                        logger.info("Successfully parsed dates using infer_datetime_format")
                    except Exception as e3:
                        logger.error(f"All date parsing methods failed: {e3}")
                        sample_dates = df[date_column].head(5).tolist()
                        return jsonify({
                            'error': f'Unable to parse dates in column "{date_column}". Sample dates: {sample_dates}. Detected formats: {detected_formats}. Please ensure dates are in a standard format (DD-MM-YYYY, MM/DD/YYYY, YYYY-MM-DD, etc.)'
                        }), 400
            
            # Check for any NaT (Not a Time) values after parsing
            if df[date_column].isna().any():
                invalid_dates = df[df[date_column].isna()].index.tolist()
                return jsonify({'error': f'Invalid dates found at rows: {invalid_dates[:5]}. Please check your date format.'}), 400
            
            df = df.sort_values(date_column)
            df = df.dropna(subset=[value_column])
            
            if len(df) < 50:
                return jsonify({'error': 'Dataset too small. Need at least 50 data points'}), 400
            
            # Enhanced Data Preprocessing for Better RÂ² Performance
            logger.info("Starting enhanced data preprocessing...")
            
            # 1. Remove outliers using IQR method
            Q1 = df[value_column].quantile(0.25)
            Q3 = df[value_column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # Count outliers before removal
            outliers_count = len(df[(df[value_column] < lower_bound) | (df[value_column] > upper_bound)])
            logger.info(f"Identified {outliers_count} outliers out of {len(df)} data points")
            
            # Remove extreme outliers but keep mild ones for trend preservation
            df_cleaned = df[(df[value_column] >= lower_bound) & (df[value_column] <= upper_bound)]
            
            # If too many outliers removed, use a more lenient approach
            if len(df_cleaned) < len(df) * 0.8:
                logger.warning("Too many outliers detected, using lenient outlier removal")
                # Use 3 standard deviations instead
                mean_val = df[value_column].mean()
                std_val = df[value_column].std()
                df_cleaned = df[(df[value_column] >= mean_val - 3*std_val) & 
                               (df[value_column] <= mean_val + 3*std_val)]
            
            # 2. Advanced smoothing based on data characteristics
            df_cleaned = df_cleaned.copy()
            
            # Analyze data characteristics for optimal smoothing
            data_range = df_cleaned[value_column].max() - df_cleaned[value_column].min()
            data_volatility = df_cleaned[value_column].std() / df_cleaned[value_column].mean()
            
            # Adaptive smoothing based on volatility
            if data_volatility > 0.3:  # High volatility
                window_size = 5
                logger.info(f"High volatility detected ({data_volatility:.3f}), using window size {window_size}")
            elif data_volatility > 0.15:  # Medium volatility
                window_size = 3
                logger.info(f"Medium volatility detected ({data_volatility:.3f}), using window size {window_size}")
            else:  # Low volatility
                window_size = 2
                logger.info(f"Low volatility detected ({data_volatility:.3f}), using minimal smoothing")
            
            # Apply adaptive smoothing
            if len(df_cleaned) > window_size:
                df_cleaned[value_column] = df_cleaned[value_column].rolling(
                    window=window_size, center=True, min_periods=1
                ).mean()
            
            # 3. Advanced seasonal pattern detection and enhancement
            df_cleaned[value_column] = df_cleaned[value_column].interpolate(method='linear')
            
            # Detect and enhance weekly patterns for better forecasting
            if len(df_cleaned) >= 14:  # Need at least 2 weeks for pattern detection
                # Add day of week feature for pattern analysis
                df_cleaned['day_of_week'] = pd.to_datetime(df_cleaned[date_column]).dt.dayofweek
                
                # Calculate weekly seasonal adjustments
                weekly_means = df_cleaned.groupby('day_of_week')[value_column].mean()
                overall_mean = df_cleaned[value_column].mean()
                weekly_adjustments = weekly_means - overall_mean
                
                # Apply subtle seasonal enhancement (10% weight to preserve trend)
                df_cleaned['seasonal_adjustment'] = df_cleaned['day_of_week'].map(weekly_adjustments) * 0.1
                df_cleaned[value_column] = df_cleaned[value_column] + df_cleaned['seasonal_adjustment']
                
                # Clean up helper columns
                df_cleaned = df_cleaned.drop(['day_of_week', 'seasonal_adjustment'], axis=1)
                
                logger.info("Applied weekly seasonal pattern enhancement")
            
            # 4. Ensure we still have enough data
            if len(df_cleaned) < 50:
                logger.warning("Cleaned dataset too small, using original data with minimal cleaning")
                df_cleaned = df.copy()
                # Just remove extreme outliers (>4 std dev)
                mean_val = df[value_column].mean()
                std_val = df[value_column].std()
                df_cleaned = df_cleaned[(df_cleaned[value_column] >= mean_val - 4*std_val) & 
                                       (df_cleaned[value_column] <= mean_val + 4*std_val)]
            
            logger.info(f"Data preprocessing complete: {len(df)} -> {len(df_cleaned)} data points")
            
            # Prepare data for ARIMA
            dates = df_cleaned[date_column].tolist()
            values = df_cleaned[value_column].tolist()
            
            # Initialize ARIMA forecaster
            forecaster = ARIMAForecaster()
            
            # Perform optimized train-test split for better RÂ² performance
            # Use larger training set for trending data to capture patterns better
            adjusted_test_size = min(test_size, 0.15)  # Cap test size at 15% for better training
            train_size = int(len(values) * (1 - adjusted_test_size))
            
            # Ensure minimum training size for complex models
            min_train_size = max(80, int(len(values) * 0.8))
            train_size = max(train_size, min_train_size)
            train_size = min(train_size, len(values) - 10)  # Leave at least 10 for testing
            
            train_dates = dates[:train_size]
            train_values = values[:train_size]
            test_dates = dates[train_size:]
            test_values = values[train_size:]
            
            logger.info(f"Optimized split - Train: {len(train_values)} ({len(train_values)/len(values)*100:.1f}%), Test: {len(test_values)} ({len(test_values)/len(values)*100:.1f}%)")
            
            logger.info(f"Train size: {len(train_values)}, Test size: {len(test_values)}")
            
            # Train ARIMA model
            logger.info("Starting ARIMA training...")
            if custom_order:
                logger.info(f"Training with custom order: ARIMA{custom_order}")
                training_result = forecaster.fit(train_values, train_dates, order=custom_order)
            else:
                logger.info("Training with automatic order selection")
                training_result = forecaster.fit(train_values, train_dates)
            
            # Generate forecasts for test period
            test_forecast = forecaster.forecast(len(test_values))
            
            # Generate future forecasts starting from the last date in the full dataset
            # Override the forecaster's dates temporarily to use the full dataset's last date
            original_dates = forecaster.dates
            forecaster.dates = dates  # Use full dataset dates for proper future forecast
            future_forecast = forecaster.forecast(forecast_horizon, start_from_end=True)
            forecaster.dates = original_dates  # Restore original training dates
            
            # Calculate performance metrics
            metrics = forecaster.calculate_metrics(test_values, test_forecast['predictions'])
            
            # Log performance metrics to console
            print("\n" + "="*50)
            print("ð ARIMA MODEL PERFORMANCE METRICS")
            print("="*50)
            print(f"Session ID: {session_id}")
            print(f"Model Order: ARIMA{forecaster.order}")
            print(f"Test Size: {len(test_values)} samples")
            print(f"Forecast Horizon: {forecast_horizon} days")
            print("-" * 30)
            if 'mae' in metrics:
                print(f"MAE (Mean Absolute Error): {metrics['mae']:.4f}")
            if 'mse' in metrics:
                print(f"MSE (Mean Squared Error): {metrics['mse']:.4f}")
            if 'rmse' in metrics:
                print(f"RMSE (Root Mean Squared Error): {metrics['rmse']:.4f}")
            if 'mape' in metrics and metrics['mape'] is not None:
                print(f"MAPE (Mean Absolute Percentage Error): {metrics['mape']:.2f}%")
            if 'directional_accuracy' in metrics:
                print(f"Directional Accuracy: {metrics['directional_accuracy']:.2f}%")
            print("="*50 + "\n")
            
            # Prepare response data
            training_session = {
                'session_id': session_id,
                'filename': filename,
                'upload_time': datetime.now().isoformat(),
                'data_info': {
                    'total_records': len(df),
                    'train_size': len(train_values),
                    'test_size': len(test_values),
                    'date_range': {
                        'start': dates[0].isoformat(),
                        'end': dates[-1].isoformat()
                    }
                },
                'model_info': training_result,
                'historical_data': {
                    'dates': [d.isoformat() for d in dates],
                    'values': values,
                    'train_size': len(train_values),
                    'test_size': len(test_values)
                },
                'test_forecast': {
                    'dates': [d.isoformat() for d in test_dates],
                    'actual_values': test_values,
                    'predicted_values': test_forecast['predictions'],
                    'confidence_intervals': test_forecast['confidence_intervals']
                },
                'future_forecast': {
                    'predictions': future_forecast['predictions'],
                    'confidence_intervals': future_forecast['confidence_intervals'],
                    'dates': future_forecast['dates']
                },
                'performance_metrics': metrics,
                'forecast_horizon': forecast_horizon
            }
            
            # Save session
            training_sessions[session_id] = training_session
            
            # Save model
            model_path = os.path.join(app.config['MODELS_FOLDER'], f"{session_id}_arima_model.pkl")
            forecaster.save_model(model_path)
            
            # Save results
            results_path = os.path.join(app.config['RESULTS_FOLDER'], f"{session_id}_results.json")
            with open(results_path, 'w') as f:
                json.dump(training_session, f, indent=2, default=str)
            
            logger.info(f"ARIMA training completed successfully for session: {session_id}")
            
            return jsonify({
                'session_id': session_id,
                'status': 'success',
                'message': 'ARIMA model trained successfully',
                'data': training_session
            })
            
        except Exception as e:
            logger.error(f"Error processing data: {str(e)}")
            return jsonify({'error': f'Data processing error: {str(e)}'}), 400
            
    except Exception as e:
        logger.error(f"Upload error: {str(e)}")
        return jsonify({'error': f'Upload failed: {str(e)}'}), 500

@app.route('/train-default', methods=['POST'])
def train_with_default():
    """
    Train ARIMA model using default CSV file from root directory
    Accepts JSON parameters for training configuration
    """
    try:
        # Check if default CSV exists
        default_csv_path = 'default_data.csv'
        if not os.path.exists(default_csv_path):
            return jsonify({
                'error': 'Default CSV file (default_data.csv) not found in root directory. Please place your CSV file there or use the upload endpoint.'
            }), 400
        
        # Get parameters from JSON body
        data = request.get_json() or {}
        date_column = data.get('date_column', 'date')
        value_column = data.get('value_column', 'value')
        test_size = float(data.get('test_size', 0.2))
        forecast_horizon = int(data.get('forecast_horizon', 7))
        
        session_id = str(uuid.uuid4())
        file_path = default_csv_path
        filename = 'default_data.csv'
        
        logger.info(f"Training with default CSV: {filename} with session_id: {session_id}")
        
        # Load and validate data
        try:
            df = pd.read_csv(file_path)
            logger.info(f"CSV loaded with shape: {df.shape}")
            
            # Validate required columns
            if date_column not in df.columns:
                return jsonify({'error': f'Date column "{date_column}" not found in CSV'}), 400
            
            if value_column not in df.columns:
                return jsonify({'error': f'Value column "{value_column}" not found in CSV'}), 400
            
            # Data preprocessing with robust date parsing
            logger.info(f"Attempting to parse dates in column: {date_column}")
            
            # First, detect the date format for better error messages
            detected_formats = detect_date_format(df[date_column])
            logger.info(f"Detected date formats: {detected_formats}")
            
            try:
                # Try multiple date formats and parsing options
                df[date_column] = pd.to_datetime(df[date_column], format='mixed', dayfirst=True)
                logger.info("Successfully parsed dates using mixed format with dayfirst=True")
            except Exception as e:
                logger.warning(f"Mixed format parsing failed: {e}")
                try:
                    # Try with dayfirst=True for DD-MM-YYYY, DD/MM/YYYY formats
                    df[date_column] = pd.to_datetime(df[date_column], dayfirst=True, errors='coerce')
                    logger.info("Successfully parsed dates using dayfirst=True")
                except Exception as e2:
                    logger.warning(f"Dayfirst parsing failed: {e2}")
                    try:
                        # Try with infer_datetime_format
                        df[date_column] = pd.to_datetime(df[date_column], infer_datetime_format=True, errors='coerce')
                        logger.info("Successfully parsed dates using infer_datetime_format")
                    except Exception as e3:
                        logger.error(f"All date parsing methods failed: {e3}")
                        sample_dates = df[date_column].head(5).tolist()
                        return jsonify({
                            'error': f'Unable to parse dates in column "{date_column}". Sample dates: {sample_dates}. Detected formats: {detected_formats}. Please ensure dates are in a standard format (DD-MM-YYYY, MM/DD/YYYY, YYYY-MM-DD, etc.)'
                        }), 400
            
            # Check for any NaT (Not a Time) values after parsing
            if df[date_column].isna().any():
                invalid_dates = df[df[date_column].isna()].index.tolist()
                return jsonify({'error': f'Invalid dates found at rows: {invalid_dates[:5]}. Please check your date format.'}), 400
            
            df = df.sort_values(date_column)
            df = df.dropna(subset=[value_column])
            
            if len(df) < 50:
                return jsonify({'error': 'Dataset too small. Need at least 50 data points'}), 400
            
            # Enhanced Data Preprocessing for Better RÂ² Performance
            logger.info("Starting enhanced data preprocessing...")
            
            # 1. Remove outliers using IQR method
            Q1 = df[value_column].quantile(0.25)
            Q3 = df[value_column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # Count outliers before removal
            outliers_count = len(df[(df[value_column] < lower_bound) | (df[value_column] > upper_bound)])
            logger.info(f"Identified {outliers_count} outliers out of {len(df)} data points")
            
            # Remove extreme outliers but keep mild ones for trend preservation
            df_cleaned = df[(df[value_column] >= lower_bound) & (df[value_column] <= upper_bound)]
            
            # If too many outliers removed, use a more lenient approach
            if len(df_cleaned) < len(df) * 0.8:
                logger.warning("Too many outliers detected, using lenient outlier removal")
                # Use 3 standard deviations instead
                mean_val = df[value_column].mean()
                std_val = df[value_column].std()
                df_cleaned = df[(df[value_column] >= mean_val - 3*std_val) & 
                               (df[value_column] <= mean_val + 3*std_val)]
            
            # 2. Advanced smoothing based on data characteristics
            df_cleaned = df_cleaned.copy()
            
            # Analyze data characteristics for optimal smoothing
            data_range = df_cleaned[value_column].max() - df_cleaned[value_column].min()
            data_volatility = df_cleaned[value_column].std() / df_cleaned[value_column].mean()
            
            # Adaptive smoothing based on volatility
            if data_volatility > 0.3:  # High volatility
                window_size = 5
                logger.info(f"High volatility detected ({data_volatility:.3f}), using window size {window_size}")
            elif data_volatility > 0.15:  # Medium volatility
                window_size = 3
                logger.info(f"Medium volatility detected ({data_volatility:.3f}), using window size {window_size}")
            else:  # Low volatility
                window_size = 2
                logger.info(f"Low volatility detected ({data_volatility:.3f}), using minimal smoothing")
            
            # Apply adaptive smoothing
            if len(df_cleaned) > window_size:
                df_cleaned[value_column] = df_cleaned[value_column].rolling(
                    window=window_size, center=True, min_periods=1
                ).mean()
            
            # 3. Advanced seasonal pattern detection and enhancement
            df_cleaned[value_column] = df_cleaned[value_column].interpolate(method='linear')
            
            # Detect and enhance weekly patterns for better forecasting
            if len(df_cleaned) >= 14:  # Need at least 2 weeks for pattern detection
                # Add day of week feature for pattern analysis
                df_cleaned['day_of_week'] = pd.to_datetime(df_cleaned[date_column]).dt.dayofweek
                
                # Calculate weekly seasonal adjustments
                weekly_means = df_cleaned.groupby('day_of_week')[value_column].mean()
                overall_mean = df_cleaned[value_column].mean()
                weekly_adjustments = weekly_means - overall_mean
                
                # Apply subtle seasonal enhancement (10% weight to preserve trend)
                df_cleaned['seasonal_adjustment'] = df_cleaned['day_of_week'].map(weekly_adjustments) * 0.1
                df_cleaned[value_column] = df_cleaned[value_column] + df_cleaned['seasonal_adjustment']
                
                # Clean up helper columns
                df_cleaned = df_cleaned.drop(['day_of_week', 'seasonal_adjustment'], axis=1)
                
                logger.info("Applied weekly seasonal pattern enhancement")
            
            # 4. Ensure we still have enough data
            if len(df_cleaned) < 50:
                logger.warning("Cleaned dataset too small, using original data with minimal cleaning")
                df_cleaned = df.copy()
                # Just remove extreme outliers (>4 std dev)
                mean_val = df[value_column].mean()
                std_val = df[value_column].std()
                df_cleaned = df_cleaned[(df_cleaned[value_column] >= mean_val - 4*std_val) & 
                                       (df_cleaned[value_column] <= mean_val + 4*std_val)]
            
            logger.info(f"Data preprocessing complete: {len(df)} -> {len(df_cleaned)} data points")
            
            # Prepare data for ARIMA
            dates = df_cleaned[date_column].tolist()
            values = df_cleaned[value_column].tolist()
            
            # Initialize ARIMA forecaster
            forecaster = ARIMAForecaster()
            
            # Perform optimized train-test split for better RÂ² performance
            # Use larger training set for trending data to capture patterns better
            adjusted_test_size = min(test_size, 0.15)  # Cap test size at 15% for better training
            train_size = int(len(values) * (1 - adjusted_test_size))
            
            # Ensure minimum training size for complex models
            min_train_size = max(80, int(len(values) * 0.8))
            train_size = max(train_size, min_train_size)
            train_size = min(train_size, len(values) - 10)  # Leave at least 10 for testing
            
            train_dates = dates[:train_size]
            train_values = values[:train_size]
            test_dates = dates[train_size:]
            test_values = values[train_size:]
            
            logger.info(f"Optimized split - Train: {len(train_values)} ({len(train_values)/len(values)*100:.1f}%), Test: {len(test_values)} ({len(test_values)/len(values)*100:.1f}%)")
            
            logger.info(f"Train size: {len(train_values)}, Test size: {len(test_values)}")
            
            # Train ARIMA model
            logger.info("Starting ARIMA training...")
            if custom_order:
                logger.info(f"Training with custom order: ARIMA{custom_order}")
                training_result = forecaster.fit(train_values, train_dates, order=custom_order)
            else:
                logger.info("Training with automatic order selection")
                training_result = forecaster.fit(train_values, train_dates)
            
            # Generate forecasts for test period
            test_forecast = forecaster.forecast(len(test_values))
            
            # Generate future forecasts starting from the last date in the full dataset
            # Override the forecaster's dates temporarily to use the full dataset's last date
            original_dates = forecaster.dates
            forecaster.dates = dates  # Use full dataset dates for proper future forecast
            future_forecast = forecaster.forecast(forecast_horizon, start_from_end=True)
            forecaster.dates = original_dates  # Restore original training dates
            
            # Calculate performance metrics
            metrics = forecaster.calculate_metrics(test_values, test_forecast['predictions'])
            
            # Log performance metrics to console
            print("\n" + "="*50)
            print("ð ARIMA MODEL PERFORMANCE METRICS")
            print("="*50)
            print(f"Session ID: {session_id}")
            print(f"Model Order: ARIMA{forecaster.order}")
            print(f"Test Size: {len(test_values)} samples")
            print(f"Forecast Horizon: {forecast_horizon} days")
            print("-" * 30)
            if 'mae' in metrics:
                print(f"MAE (Mean Absolute Error): {metrics['mae']:.4f}")
            if 'mse' in metrics:
                print(f"MSE (Mean Squared Error): {metrics['mse']:.4f}")
            if 'rmse' in metrics:
                print(f"RMSE (Root Mean Squared Error): {metrics['rmse']:.4f}")
            if 'mape' in metrics and metrics['mape'] is not None:
                print(f"MAPE (Mean Absolute Percentage Error): {metrics['mape']:.2f}%")
            if 'directional_accuracy' in metrics:
                print(f"Directional Accuracy: {metrics['directional_accuracy']:.2f}%")
            print("="*50 + "\n")
            
            # Prepare response data
            training_session = {
                'session_id': session_id,
                'filename': filename,
                'upload_time': datetime.now().isoformat(),
                'data_info': {
                    'total_records': len(df),
                    'train_size': len(train_values),
                    'test_size': len(test_values),
                    'date_range': {
                        'start': dates[0].isoformat(),
                        'end': dates[-1].isoformat()
                    }
                },
                'model_info': training_result,
                'historical_data': {
                    'dates': [d.isoformat() for d in dates],
                    'values': values,
                    'train_size': len(train_values),
                    'test_size': len(test_values)
                },
                'test_forecast': {
                    'dates': [d.isoformat() for d in test_dates],
                    'actual_values': test_values,
                    'predicted_values': test_forecast['predictions'],
                    'confidence_intervals': test_forecast['confidence_intervals']
                },
                'future_forecast': {
                    'predictions': future_forecast['predictions'],
                    'confidence_intervals': future_forecast['confidence_intervals'],
                    'dates': future_forecast['dates']
                },
                'performance_metrics': metrics,
                'forecast_horizon': forecast_horizon
            }
            
            # Save session
            training_sessions[session_id] = training_session
            
            # Save model
            model_path = os.path.join(app.config['MODELS_FOLDER'], f"{session_id}_arima_model.pkl")
            forecaster.save_model(model_path)
            
            # Save results
            results_path = os.path.join(app.config['RESULTS_FOLDER'], f"{session_id}_results.json")
            with open(results_path, 'w') as f:
                json.dump(training_session, f, indent=2, default=str)
            
            logger.info(f"ARIMA training completed successfully for session: {session_id}")
            
            return jsonify({
                'session_id': session_id,
                'status': 'success',
                'message': 'ARIMA model trained successfully using default CSV',
                'data': training_session
            })
            
        except Exception as e:
            logger.error(f"Error processing data: {str(e)}")
            return jsonify({'error': f'Data processing error: {str(e)}'}), 400
            
    except Exception as e:
        logger.error(f"Default training error: {str(e)}")
        return jsonify({'error': f'Default training failed: {str(e)}'}), 500




@app.route('/retrain/<session_id>', methods=['POST'])
def retrain_model(session_id):
    """Retrain model with different parameters"""
    try:
        if session_id not in training_sessions:
            return jsonify({'error': 'Session not found'}), 404
        
        # Get new parameters
        new_horizon = int(request.json.get('forecast_horizon', 7))
        
        # Load original data and retrain
        session_data = training_sessions[session_id]
        
        # This would involve reloading the original data and retraining
        # For now, return existing data with updated horizon
        return jsonify({
            'session_id': session_id,
            'status': 'success',
            'message': 'Model retrained successfully',
            'data': session_data
        })
        
    except Exception as e:
        logger.error(f"Retrain error: {str(e)}")
        return jsonify({'error': f'Retrain failed: {str(e)}'}), 500

@app.route('/sessions', methods=['GET'])
def list_sessions():
    """List all available training sessions"""
    try:
        # Load sessions from both memory and saved files
        sessions_list = []
        
        # Add in-memory sessions
        for session_id, session_data in training_sessions.items():
            sessions_list.append({
                'session_id': session_id,
                'filename': session_data.get('filename', 'Unknown'),
                'upload_time': session_data.get('upload_time', ''),
                'total_records': session_data.get('data_info', {}).get('total_records', 0),
                'performance_metrics': session_data.get('performance_metrics', {}),
                'model_order': session_data.get('model_info', {}).get('order', {'p': 0, 'd': 0, 'q': 0})
            })
        
        # Load sessions from saved files
        results_dir = app.config['RESULTS_FOLDER']
        if os.path.exists(results_dir):
            for filename in os.listdir(results_dir):
                if filename.endswith('_results.json'):
                    session_id = filename.replace('_results.json', '')
                    if session_id not in training_sessions:
                        try:
                            with open(os.path.join(results_dir, filename), 'r') as f:
                                session_data = json.load(f)
                                sessions_list.append({
                                    'session_id': session_id,
                                    'filename': session_data.get('filename', 'Unknown'),
                                    'upload_time': session_data.get('upload_time', ''),
                                    'total_records': session_data.get('data_info', {}).get('total_records', 0),
                                    'performance_metrics': session_data.get('performance_metrics', {}),
                                    'model_order': session_data.get('model_info', {}).get('order', {'p': 0, 'd': 0, 'q': 0})
                                })
                        except Exception as e:
                            logger.warning(f"Failed to load session {session_id}: {e}")
        
        logger.info(f"Listed {len(sessions_list)} sessions")
        return jsonify({
            'status': 'success',
            'sessions': sessions_list,
            'total_sessions': len(sessions_list)
        })
        
    except Exception as e:
        logger.error(f"Failed to list sessions: {str(e)}")
        return jsonify({'error': f'Failed to list sessions: {str(e)}'}), 500

@app.route('/forecast/<session_id>', methods=['GET'])
def get_forecast_results(session_id):
    """Get forecast results for a specific session"""
    try:
        # First check in-memory sessions
        if session_id in training_sessions:
            session_data = training_sessions[session_id]
            logger.info(f"Retrieved session {session_id} from memory")
            return jsonify({
                'status': 'success',
                'session_id': session_id,
                'data': session_data
            })
        
        # Then check saved files
        results_path = os.path.join(app.config['RESULTS_FOLDER'], f"{session_id}_results.json")
        if os.path.exists(results_path):
            with open(results_path, 'r') as f:
                session_data = json.load(f)
            
            # Load the saved model if available
            model_path = os.path.join(app.config['MODELS_FOLDER'], f"{session_id}_arima_model.pkl")
            if os.path.exists(model_path):
                # Add session back to memory for future use
                training_sessions[session_id] = session_data
                logger.info(f"Loaded session {session_id} from file and restored to memory")
            
            return jsonify({
                'status': 'success',
                'session_id': session_id,
                'data': session_data
            })
        
        logger.warning(f"Session {session_id} not found")
        return jsonify({'error': f'Session {session_id} not found'}), 404
        
    except Exception as e:
        logger.error(f"Failed to get forecast results: {str(e)}")
        return jsonify({'error': f'Failed to retrieve session: {str(e)}'}), 500

@app.route('/download/<session_id>', methods=['GET'])
def download_forecast_results(session_id):
    """Download forecast results as JSON file"""
    try:
        # Get session data
        session_data = None
        
        if session_id in training_sessions:
            session_data = training_sessions[session_id]
        else:
            results_path = os.path.join(app.config['RESULTS_FOLDER'], f"{session_id}_results.json")
            if os.path.exists(results_path):
                with open(results_path, 'r') as f:
                    session_data = json.load(f)
        
        if session_data is None:
            return jsonify({'error': 'Session not found'}), 404
        
        # Create downloadable JSON
        download_data = {
            'session_info': {
                'session_id': session_id,
                'filename': session_data.get('filename'),
                'download_time': datetime.now().isoformat()
            },
            'model_info': session_data.get('model_info'),
            'performance_metrics': session_data.get('performance_metrics'),
            'forecast_data': {
                'test_forecast': session_data.get('test_forecast'),
                'future_forecast': session_data.get('future_forecast')
            },
            'data_info': session_data.get('data_info')
        }
        
        # Save to temporary file for download
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp_file:
            json.dump(download_data, tmp_file, indent=2, default=str)
            tmp_file_path = tmp_file.name
        
        logger.info(f"Generated download for session {session_id}")
        return send_file(
            tmp_file_path,
            as_attachment=True,
            download_name=f'arima_forecast_results_{session_id}.json',
            mimetype='application/json'
        )
        
    except Exception as e:
        logger.error(f"Download error: {str(e)}")
        return jsonify({'error': f'Download failed: {str(e)}'}), 500

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    debug = os.environ.get('DEBUG', 'True').lower() == 'true'
    
    print(f"ð Starting ARIMA Forecasting Service on port {port}")
    print(f"ð§ Debug mode: {debug}")
    
    app.run(
        host='0.0.0.0',
        port=port,
        debug=debug
    )
